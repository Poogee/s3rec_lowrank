{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e21cea2",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing Demo\n",
    "\n",
    "This notebook demonstrates the data preprocessing pipeline for the Amazon Beauty dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f9fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57d709",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Data\n",
    "\n",
    "We have two raw files:\n",
    "- `reviews_Beauty_5.json` ‚Äî user-item interactions with timestamps\n",
    "- `meta_Beauty.json` ‚Äî item metadata with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9103d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Files:\n",
      "  Reviews: True (../../reviews_Beauty_5.json)\n",
      "  Metadata: True (../../meta_Beauty.json)\n"
     ]
    }
   ],
   "source": [
    "# Check available raw data\n",
    "raw_dir = Path('../../')  # Root of recsyska_imba\n",
    "\n",
    "reviews_file = raw_dir / 'reviews_Beauty_5.json'\n",
    "meta_file = raw_dir / 'meta_Beauty.json'\n",
    "\n",
    "print(\"Raw Data Files:\")\n",
    "print(f\"  Reviews: {reviews_file.exists()} ({reviews_file})\")\n",
    "print(f\"  Metadata: {meta_file.exists()} ({meta_file})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e76421",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Sample Review Entry:\n",
      "  reviewerID: A1YJEY40YUW4SE\n",
      "  asin: 7806397051\n",
      "  reviewerName: Andrea\n",
      "  helpful: [3, 4]\n",
      "  reviewText: Very oily and creamy. Not at all what I expected... ordered this to try to highlight and contour and it just looked awful!!! Plus, took FOREVER to arrive.\n",
      "  overall: 1.0\n",
      "  summary: Don't waste your money\n",
      "  unixReviewTime: 1391040000\n",
      "  reviewTime: 01 30, 2014\n"
     ]
    }
   ],
   "source": [
    "# Preview reviews\n",
    "if reviews_file.exists():\n",
    "    print(\"\\nüìã Sample Review Entry:\")\n",
    "    with open(reviews_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 1:\n",
    "                review = json.loads(line)\n",
    "                for k, v in review.items():\n",
    "                    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53f9ca",
   "metadata": {},
   "source": [
    "## Step 2: K-core Filtering\n",
    "\n",
    "We apply k-core filtering to ensure:\n",
    "- Each user has at least K interactions\n",
    "- Each item appears in at least K interactions\n",
    "\n",
    "This removes very sparse users and unpopular items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f46cec3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: 1000 users, 500 items, 5000 interactions\n",
      "  Iter 1: 552 users, 481 items, 3658 interactions\n",
      "  Iter 2: 547 users, 432 items, 3461 interactions\n",
      "  Iter 3: 507 users, 431 items, 3301 interactions\n",
      "  Iter 4: 507 users, 419 items, 3254 interactions\n",
      "  Iter 5: 492 users, 419 items, 3194 interactions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(492, 419, 3194)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_kcore_filtering(n_users=1000, n_items=500, n_interactions=5000, k=5):\n",
    "    \"\"\"Simulate k-core filtering process\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate random interactions\n",
    "    users = np.random.randint(0, n_users, n_interactions)\n",
    "    items = np.random.randint(0, n_items, n_interactions)\n",
    "    \n",
    "    print(f\"Initial: {n_users} users, {n_items} items, {n_interactions} interactions\")\n",
    "    \n",
    "    # K-core iterations\n",
    "    for iteration in range(5):\n",
    "        user_counts = Counter(users)\n",
    "        item_counts = Counter(items)\n",
    "        \n",
    "        # Filter\n",
    "        valid_mask = np.array([\n",
    "            user_counts[u] >= k and item_counts[i] >= k \n",
    "            for u, i in zip(users, items)\n",
    "        ])\n",
    "        \n",
    "        users = users[valid_mask]\n",
    "        items = items[valid_mask]\n",
    "        \n",
    "        remaining_users = len(set(users))\n",
    "        remaining_items = len(set(items))\n",
    "        remaining_interactions = len(users)\n",
    "        \n",
    "        print(f\"  Iter {iteration+1}: {remaining_users} users, {remaining_items} items, {remaining_interactions} interactions\")\n",
    "        \n",
    "        if valid_mask.all():\n",
    "            print(\"  Converged!\")\n",
    "            break\n",
    "    \n",
    "    return remaining_users, remaining_items, remaining_interactions\n",
    "\n",
    "simulate_kcore_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d17b8a",
   "metadata": {},
   "source": [
    "## Step 3: Sequence Building\n",
    "\n",
    "For each user, we sort their interactions by timestamp to create a chronological sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b89cf29",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw interactions (unsorted):\n",
      "  Item: A, Time: 1420000000\n",
      "  Item: C, Time: 1420500000\n",
      "  Item: B, Time: 1420100000\n",
      "  Item: D, Time: 1421000000\n",
      "\n",
      "Sorted sequence:\n",
      "  A ‚Üí B ‚Üí C ‚Üí D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of sequence building\n",
    "def demo_sequence_building():\n",
    "    \"\"\"Demonstrate how user sequences are built\"\"\"\n",
    "    \n",
    "    # Example user interactions\n",
    "    user_interactions = [\n",
    "        {'item': 'A', 'timestamp': 1420000000, 'rating': 5},\n",
    "        {'item': 'C', 'timestamp': 1420500000, 'rating': 4},\n",
    "        {'item': 'B', 'timestamp': 1420100000, 'rating': 3},\n",
    "        {'item': 'D', 'timestamp': 1421000000, 'rating': 5},\n",
    "    ]\n",
    "    \n",
    "    print(\"Raw interactions (unsorted):\")\n",
    "    for i in user_interactions:\n",
    "        print(f\"  Item: {i['item']}, Time: {i['timestamp']}\")\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    sorted_interactions = sorted(user_interactions, key=lambda x: x['timestamp'])\n",
    "    \n",
    "    print(\"\\nSorted sequence:\")\n",
    "    sequence = [i['item'] for i in sorted_interactions]\n",
    "    print(f\"  {' ‚Üí '.join(sequence)}\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "demo_sequence_building()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98635e",
   "metadata": {},
   "source": [
    "## Step 4: ID Mapping\n",
    "\n",
    "Convert string IDs to integer indices for neural network processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b220402",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item to ID Mapping:\n",
      "  B001ABC123 ‚Üí 1\n",
      "  B002XYZ789 ‚Üí 2\n",
      "  B003DEF456 ‚Üí 3\n",
      "\n",
      "Original: ['B001ABC123', 'B002XYZ789', 'B003DEF456', 'B001ABC123', 'B002XYZ789']\n",
      "Mapped:   [1, 2, 3, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B001ABC123': 1, 'B002XYZ789': 2, 'B003DEF456': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def demo_id_mapping():\n",
    "    \"\"\"Demonstrate ID mapping\"\"\"\n",
    "    \n",
    "    # Example items\n",
    "    items = ['B001ABC123', 'B002XYZ789', 'B003DEF456', 'B001ABC123', 'B002XYZ789']\n",
    "    \n",
    "    # Create mapping\n",
    "    unique_items = sorted(set(items))\n",
    "    item2id = {item: idx + 1 for idx, item in enumerate(unique_items)}  # 0 reserved for padding\n",
    "    id2item = {v: k for k, v in item2id.items()}\n",
    "    \n",
    "    print(\"Item to ID Mapping:\")\n",
    "    for item, idx in item2id.items():\n",
    "        print(f\"  {item} ‚Üí {idx}\")\n",
    "    \n",
    "    # Map items\n",
    "    mapped = [item2id[i] for i in items]\n",
    "    print(f\"\\nOriginal: {items}\")\n",
    "    print(f\"Mapped:   {mapped}\")\n",
    "    \n",
    "    return item2id\n",
    "\n",
    "demo_id_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c539b2",
   "metadata": {},
   "source": [
    "## Step 5: Attribute Extraction\n",
    "\n",
    "Extract attributes from item metadata (brand, categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9059d5eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Metadata:\n",
      "  asin: B001ABC123\n",
      "  brand: L'Oreal\n",
      "  categories: [['Beauty', 'Makeup', 'Lipstick']]\n",
      "  title: L'Oreal Paris Colour Riche Lipstick\n",
      "\n",
      "Extracted Attributes (4):\n",
      "  - brand:L'Oreal\n",
      "  - cat:Beauty\n",
      "  - cat:Makeup\n",
      "  - cat:Lipstick\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"brand:L'Oreal\", 'cat:Beauty', 'cat:Makeup', 'cat:Lipstick']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def demo_attribute_extraction():\n",
    "    \"\"\"Demonstrate attribute extraction from metadata\"\"\"\n",
    "    \n",
    "    # Example metadata\n",
    "    sample_metadata = {\n",
    "        'asin': 'B001ABC123',\n",
    "        'brand': 'L\\'Oreal',\n",
    "        'categories': [['Beauty', 'Makeup', 'Lipstick']],\n",
    "        'title': 'L\\'Oreal Paris Colour Riche Lipstick'\n",
    "    }\n",
    "    \n",
    "    print(\"Sample Metadata:\")\n",
    "    for k, v in sample_metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Extract attributes\n",
    "    attributes = []\n",
    "    \n",
    "    # Add brand\n",
    "    if 'brand' in sample_metadata and sample_metadata['brand']:\n",
    "        attributes.append(f\"brand:{sample_metadata['brand']}\")\n",
    "    \n",
    "    # Add categories\n",
    "    if 'categories' in sample_metadata:\n",
    "        for cat_list in sample_metadata['categories']:\n",
    "            for cat in cat_list:\n",
    "                attributes.append(f\"cat:{cat}\")\n",
    "    \n",
    "    print(f\"\\nExtracted Attributes ({len(attributes)}):\")\n",
    "    for attr in attributes:\n",
    "        print(f\"  - {attr}\")\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "demo_attribute_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff5b8c",
   "metadata": {},
   "source": [
    "## Step 6: Train/Val/Test Split\n",
    "\n",
    "We use leave-one-out splitting:\n",
    "- Last item ‚Üí Test\n",
    "- Second-to-last ‚Üí Validation\n",
    "- Rest ‚Üí Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d35c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Sequence: A ‚Üí B ‚Üí C ‚Üí D ‚Üí E ‚Üí F ‚Üí G ‚Üí H\n",
      "Length: 8\n",
      "\n",
      "Leave-One-Out Split:\n",
      "  Train Input:  ['A', 'B', 'C', 'D', 'E', 'F']\n",
      "  Val Input:    ['A', 'B', 'C', 'D', 'E', 'F']\n",
      "  Val Target:   G\n",
      "  Test Input:   ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
      "  Test Target:  H\n"
     ]
    }
   ],
   "source": [
    "def demo_train_val_test_split():\n",
    "    \"\"\"Demonstrate leave-one-out splitting\"\"\"\n",
    "    \n",
    "    # Example sequence\n",
    "    sequence = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    \n",
    "    print(f\"Full Sequence: {' ‚Üí '.join(sequence)}\")\n",
    "    print(f\"Length: {len(sequence)}\")\n",
    "    print()\n",
    "    \n",
    "    # Split\n",
    "    train = sequence[:-2]\n",
    "    val_input = sequence[:-2]\n",
    "    val_target = sequence[-2]\n",
    "    test_input = sequence[:-1]\n",
    "    test_target = sequence[-1]\n",
    "    \n",
    "    print(\"Leave-One-Out Split:\")\n",
    "    print(f\"  Train Input:  {train}\")\n",
    "    print(f\"  Val Input:    {val_input}\")\n",
    "    print(f\"  Val Target:   {val_target}\")\n",
    "    print(f\"  Test Input:   {test_input}\")\n",
    "    print(f\"  Test Target:  {test_target}\")\n",
    "    \n",
    "demo_train_val_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3746efd",
   "metadata": {},
   "source": [
    "## Step 7: Final Processed Data\n",
    "\n",
    "The preprocessing outputs a pickle file with:\n",
    "- User sequences (mapped to integer IDs)\n",
    "- Item-to-attributes mapping\n",
    "- Various statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aefa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed Data Summary:\n",
      "  Users: 22,363\n",
      "  Items: 12,102\n",
      "  Attributes: 2,320\n",
      "  Avg sequence length: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Check if processed data exists\n",
    "processed_path = Path('../data/processed/beauty_processed.pkl')\n",
    "\n",
    "if processed_path.exists():\n",
    "    with open(processed_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Processed Data Summary:\")\n",
    "    print(f\"  Users: {data['num_users']:,}\")\n",
    "    print(f\"  Items: {data['num_items']:,}\")\n",
    "    print(f\"  Attributes: {data['num_attributes']:,}\")\n",
    "    print(f\"  Avg sequence length: {np.mean([len(s) for s in data['user_sequences'] if s]):.1f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Processed data not found. Run preprocessing first:\")\n",
    "    print(\"  python experiments/preprocess.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00670cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Summary\n",
    "\n",
    "The preprocessing pipeline:\n",
    "1. **Load** raw JSON files\n",
    "2. **Filter** with k-core algorithm (k=5)\n",
    "3. **Build** chronological sequences per user\n",
    "4. **Map** string IDs to integers\n",
    "5. **Extract** attributes (brand, categories)\n",
    "6. **Split** using leave-one-out\n",
    "7. **Save** as pickle for fast loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
