{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d949415",
   "metadata": {},
   "source": [
    "# 3. Model Architecture Visualization\n",
    "\n",
    "This notebook visualizes the S3Rec model architecture and its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d496597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab08bbee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Encoder' from 'models.modules' (/Users/poogee/recsyska_imba/s3rec_lowrank/notebooks/../models/modules.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlowrank_aap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LowRankAAP, FullRankAAP\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms3rec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S3RecModel, S3RecLowRankModel\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Encoder' from 'models.modules' (/Users/poogee/recsyska_imba/s3rec_lowrank/notebooks/../models/modules.py)"
     ]
    }
   ],
   "source": [
    "from models.lowrank_aap import LowRankAAP, FullRankAAP\n",
    "from models.modules import Encoder\n",
    "from models.s3rec import S3RecModel, S3RecLowRankModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e143a",
   "metadata": {},
   "source": [
    "## 1. S3Rec Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1797ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def draw_architecture():\n",
    "    \"\"\"Draw the S3Rec architecture diagram\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(0, 16)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Colors\n",
    "    colors = {\n",
    "        'embedding': '#E3F2FD',\n",
    "        'transformer': '#E8F5E9',\n",
    "        'pretrain': '#FFF3E0',\n",
    "        'finetune': '#FCE4EC',\n",
    "        'lowrank': '#F3E5F5'\n",
    "    }\n",
    "    \n",
    "    # Title\n",
    "    ax.text(8, 9.5, 'S3Rec Architecture with Low-rank AAP', \n",
    "            fontsize=16, fontweight='bold', ha='center')\n",
    "    \n",
    "    # Input Layer\n",
    "    ax.add_patch(FancyBboxPatch((1, 7.5), 3, 1, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors['embedding'], edgecolor='black'))\n",
    "    ax.text(2.5, 8, 'Input Sequence\\n[v₁, v₂, ..., vₙ]', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Embedding Layer\n",
    "    ax.add_patch(FancyBboxPatch((1, 5.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors['embedding'], edgecolor='black'))\n",
    "    ax.text(2.5, 6.25, 'Embedding Layer\\n• Item Embeddings\\n• Position Embeddings', \n",
    "            ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(2.5, 5.5), xytext=(2.5, 7.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    ax.add_patch(FancyBboxPatch((1, 2.5), 3, 2.5, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors['transformer'], edgecolor='black', linewidth=2))\n",
    "    ax.text(2.5, 4.5, 'Transformer Encoder', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(2.5, 3.8, '• Multi-Head Attention', ha='center', va='center', fontsize=9)\n",
    "    ax.text(2.5, 3.3, '• Feed-Forward Network', ha='center', va='center', fontsize=9)\n",
    "    ax.text(2.5, 2.8, '• Layer Normalization', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(2.5, 2.5), xytext=(2.5, 5.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    \n",
    "    # Pre-training tasks (right side)\n",
    "    ax.text(10, 9, 'Pre-training Tasks', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    pretrain_tasks = [\n",
    "        ('AAP', 'Associated Attribute\\nPrediction'),\n",
    "        ('MIP', 'Masked Item\\nPrediction'),\n",
    "        ('MAP', 'Masked Attribute\\nPrediction'),\n",
    "        ('SP', 'Segment\\nPrediction')\n",
    "    ]\n",
    "    \n",
    "    for i, (name, desc) in enumerate(pretrain_tasks):\n",
    "        y_pos = 7.5 - i * 1.5\n",
    "        ax.add_patch(FancyBboxPatch((7, y_pos), 2, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                                     facecolor=colors['pretrain'], edgecolor='black'))\n",
    "        ax.text(8, y_pos + 0.6, f'{name}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        ax.text(8, y_pos + 0.2, desc, ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Low-rank AAP highlight\n",
    "    ax.add_patch(FancyBboxPatch((10, 7.5), 4, 1.2, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors['lowrank'], edgecolor='purple', linewidth=2))\n",
    "    ax.text(12, 8.1, 'Low-rank AAP (OURS)', ha='center', va='center', fontsize=10, fontweight='bold', color='purple')\n",
    "    ax.text(12, 7.7, 'W ≈ U × Vᵀ, rank = r', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrow from pretrain tasks\n",
    "    ax.annotate('', xy=(7, 5), xytext=(4.2, 3.75),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', linestyle='--'))\n",
    "    \n",
    "    # Fine-tuning (bottom)\n",
    "    ax.add_patch(FancyBboxPatch((1, 0.5), 3, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor=colors['finetune'], edgecolor='black'))\n",
    "    ax.text(2.5, 1.25, 'Fine-tuning\\nNext Item Prediction', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Arrow\n",
    "    ax.annotate('', xy=(2.5, 0.5), xytext=(2.5, 2.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='black'))\n",
    "    \n",
    "    # Legend\n",
    "    legend_items = [\n",
    "        (colors['embedding'], 'Input/Embedding'),\n",
    "        (colors['transformer'], 'Transformer'),\n",
    "        (colors['pretrain'], 'Pre-training'),\n",
    "        (colors['finetune'], 'Fine-tuning'),\n",
    "        (colors['lowrank'], 'Our Contribution')\n",
    "    ]\n",
    "    \n",
    "    for i, (color, label) in enumerate(legend_items):\n",
    "        ax.add_patch(Rectangle((12, 5 - i * 0.5), 0.4, 0.3, facecolor=color, edgecolor='black'))\n",
    "        ax.text(12.6, 5.15 - i * 0.5, label, fontsize=9, va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/architecture.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "draw_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6c3ce",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad7e54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_attention(seq_len=5):\n",
    "    \"\"\"Visualize self-attention patterns\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Bidirectional (pre-training)\n",
    "    ax1 = axes[0]\n",
    "    attention = np.ones((seq_len, seq_len))\n",
    "    im1 = ax1.imshow(attention, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax1.set_title('Bidirectional Attention\\n(Pre-training)', fontsize=12)\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    ax1.set_xticks(range(seq_len))\n",
    "    ax1.set_yticks(range(seq_len))\n",
    "    ax1.set_xticklabels([f'v{i+1}' for i in range(seq_len)])\n",
    "    ax1.set_yticklabels([f'v{i+1}' for i in range(seq_len)])\n",
    "    \n",
    "    # Unidirectional (fine-tuning)\n",
    "    ax2 = axes[1]\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    im2 = ax2.imshow(mask, cmap='Oranges', vmin=0, vmax=1)\n",
    "    ax2.set_title('Unidirectional (Causal) Attention\\n(Fine-tuning)', fontsize=12)\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    ax2.set_xticks(range(seq_len))\n",
    "    ax2.set_yticks(range(seq_len))\n",
    "    ax2.set_xticklabels([f'v{i+1}' for i in range(seq_len)])\n",
    "    ax2.set_yticklabels([f'v{i+1}' for i in range(seq_len)])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/attention_patterns.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Bidirectional: Each position can attend to ALL positions\")\n",
    "    print(\"Unidirectional: Each position can only attend to PREVIOUS positions\")\n",
    "\n",
    "visualize_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177c0d8",
   "metadata": {},
   "source": [
    "## 3. Low-rank AAP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69743d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lowrank_decomposition(d=64, r=16):\n",
    "    \"\"\"Visualize the low-rank decomposition\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    # Full-rank W\n",
    "    ax1 = axes[0]\n",
    "    W = np.random.randn(d, d) * 0.1\n",
    "    im1 = ax1.imshow(W, cmap='RdBu', vmin=-0.3, vmax=0.3)\n",
    "    ax1.set_title(f'Full-rank W\\n{d}×{d} = {d*d:,} params', fontsize=11)\n",
    "    ax1.set_xlabel('Dimension')\n",
    "    ax1.set_ylabel('Dimension')\n",
    "    plt.colorbar(im1, ax=ax1, shrink=0.6)\n",
    "    \n",
    "    # U matrix\n",
    "    ax2 = axes[1]\n",
    "    U = np.random.randn(d, r) * 0.1\n",
    "    V = np.random.randn(d, r) * 0.1\n",
    "    \n",
    "    # Combined visualization\n",
    "    combined = np.zeros((d, d))\n",
    "    combined[:, :r] = U * 2  # Scale for visibility\n",
    "    \n",
    "    ax2.imshow(U, cmap='Greens', vmin=-0.3, vmax=0.3)\n",
    "    ax2.set_title(f'U matrix\\n{d}×{r} = {d*r:,} params', fontsize=11)\n",
    "    ax2.set_xlabel('Rank')\n",
    "    ax2.set_ylabel('Dimension')\n",
    "    \n",
    "    # V matrix\n",
    "    ax3 = axes[2]\n",
    "    ax3.imshow(V.T, cmap='Purples', vmin=-0.3, vmax=0.3)\n",
    "    ax3.set_title(f'V matrix\\n{d}×{r} = {d*r:,} params', fontsize=11)\n",
    "    ax3.set_xlabel('Dimension')\n",
    "    ax3.set_ylabel('Rank')\n",
    "    \n",
    "    fig.suptitle(f'Low-rank Decomposition: W ≈ U × Vᵀ\\n'\n",
    "                 f'Parameters: {d*d:,} → {2*d*r:,} ({100*(1-2*r/d):.0f}% reduction)',\n",
    "                 fontsize=12, fontweight='bold', y=1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/lowrank_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_lowrank_decomposition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0054e9",
   "metadata": {},
   "source": [
    "## 4. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c46f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Model instantiation and summary\n",
    "num_items = 12102\n",
    "num_attrs = 1221\n",
    "hidden_size = 64\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"S3Rec Model Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Vocabulary:\")\n",
    "print(f\"    Items: {num_items:,}\")\n",
    "print(f\"    Attributes: {num_attrs:,}\")\n",
    "print(f\"  Architecture:\")\n",
    "print(f\"    Hidden size: {hidden_size}\")\n",
    "print(f\"    Num layers: 2\")\n",
    "print(f\"    Num heads: 2\")\n",
    "print(f\"    Max sequence: 50\")\n",
    "print()\n",
    "\n",
    "# Parameter comparison\n",
    "print(\"=\"*60)\n",
    "print(\"Parameter Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline = S3RecModel(num_items, num_attrs, hidden_size=hidden_size)\n",
    "baseline_params = sum(p.numel() for p in baseline.parameters())\n",
    "print(f\"  Baseline S3Rec:     {baseline_params:,} parameters\")\n",
    "\n",
    "for rank in [8, 16, 32]:\n",
    "    model = S3RecLowRankModel(num_items, num_attrs, hidden_size=hidden_size, rank=rank)\n",
    "    model_params = sum(p.numel() for p in model.parameters())\n",
    "    reduction = (baseline_params - model_params) / baseline_params * 100\n",
    "    print(f\"  Low-rank (r={rank:2d}):    {model_params:,} parameters ({reduction:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab722a1e",
   "metadata": {},
   "source": [
    "## 5. Layer-by-Layer Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f05a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_breakdown(model, name=\"Model\"):\n",
    "    \"\"\"Show parameters per layer\"\"\"\n",
    "    print(f\"\\n{name} - Layer Breakdown:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total = 0\n",
    "    for layer_name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            n = param.numel()\n",
    "            total += n\n",
    "            # Shorten layer names\n",
    "            short_name = layer_name.replace('transformer_encoder.', 'enc.')\n",
    "            short_name = short_name.replace('.weight', '.W')\n",
    "            short_name = short_name.replace('.bias', '.b')\n",
    "            if n > 1000:\n",
    "                print(f\"  {short_name:<40} {n:>10,}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  {'TOTAL':<40} {total:>10,}\")\n",
    "    \n",
    "baseline = S3RecModel(12102, 1221, hidden_size=64)\n",
    "layer_breakdown(baseline, \"Baseline S3Rec\")\n",
    "\n",
    "lowrank = S3RecLowRankModel(12102, 1221, hidden_size=64, rank=16)\n",
    "layer_breakdown(lowrank, \"Low-rank S3Rec (r=16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7878b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Summary\n",
    "\n",
    "Key architectural insights:\n",
    "1. **Transformer Encoder** - Main sequence processing component\n",
    "2. **Bidirectional vs Unidirectional** - Pre-training vs Fine-tuning attention patterns  \n",
    "3. **Low-rank AAP** - Reduces parameters while maintaining performance\n",
    "4. **Item & Attribute Embeddings** - Majority of parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
